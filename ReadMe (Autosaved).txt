Some of the scripts included in this set are drafts that I set aside
based on discussions we've had over e-mail.  The CORE section below covers
the OCR evaluator, and the DRAFT section those that I set aside. 

CORE SCRIPTS:

Dictionary.py: reads all files in a directory and compiles the first word
from each line into a set.

FSBuilder.py: accepts a set of words (ideally generated by Dictionary.py)
and produces a set contained every word that has non-final s's, substitutes
f's for all non-final s's.

TokenGen.py: accepts a list of lines and cleans them for processing by
breaking them into potential tokens, stripping them of punctuation,
and discarding invalid tokens.  Discarding happens twice, once before 
punctuation is removed to catch tags and again after punctuation is removed
to catch numbers.  A second similar function can be called from this
module to try fuse words at the end of a line that were broken by a
hyphen (not used by default, see below).  I mostly included it here with
an eye towards having multiple functions in TokenGen to be used for
different levels of scrutiny or correction later in the project.

AccEval.py: accepts at minimum a cleaned list of words and a dictionary.  
Optionally, it will accept a list of substitution rules and a command to 
use the hyphen correction version of the token generator (only used if
you specifically pass in a boolean for it).  Checks all tokens against
the dictionary and substitution rules (if passed in) and returns a six-
value tuple with total capitalized tokens, total capitalized dictionary
matches, total capitalized matches by substitution, total lower-case 
tokens, total lower-case dictionary matches, and total lower-cas matches
by substitution.

These are the four core modules.  Dictionary's and FSBuilder's are designed
to be run once per batch process to load all potential matches into memory.
AccEval doesn't run them, it only accepts sets that should be generated
with them.  

To see how they were designed to interact with one another, look at and/or
run either SingleTest.py or BatchTest.py.  These are just quick scripts I
put together to help me test my functions after I started separating them
into modules.  In both, Dictionary/FSBuilder are run once at the beginning,
and their results are passed into AccEval along with a list of lines.
AccEval calls TokenGen to process the list of lines and then compares the
list of processed tokens it returns to the Dictionary/FSBuilder sets that
were passed into it.  SingleTest doesn't do anything more than dump
the tuple into the shell, but BatchTest produces a short report.  I mainly
did this just so I could see how tweaks to my evaluation algorithms were
behaving across the samples.

DRAFTS/:

I originally wrote my token generator to set all characters to lower case,
mostly because I saw you doing that in a lot of your old scripts.  If you
want to run them, they'll need to be moved out of the drafts directory
because they depend on Dictionary and FSBuilder.

The two hyphenread scripts are two methods of fusing I played around with.
The first checks for a match before fusing, the other fuses automatically.
I included a version of the first as a separate function in TokenGen (that,
again, AccEval won't use unless specifically told to).

Other scripts in that directory are probably only of use to anyone who
later wants to learn how to do this sort of thing.  As examples, they're
really simple and modular. 